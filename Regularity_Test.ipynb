{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f61348e-ccf1-4108-9eec-49b1729b2323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 17:38:36.205594: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-23 17:38:36.213709: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-23 17:38:36.222505: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-23 17:38:36.225071: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-23 17:38:36.232194: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-23 17:38:39.315105: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataset.semi import SemiDataset\n",
    "from model.semseg.dpt import DPT\n",
    "from model.semseg.dpt import DPT_with_Feature\n",
    "from supervised import evaluate\n",
    "from util.classes import CLASSES\n",
    "from util.ohem import ProbOhemCrossEntropy2d\n",
    "from util.utils import count_params, init_log, AverageMeter\n",
    "from util.dist_helper import setup_distributed\n",
    "from Regularization_losses import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ac9dfe3-debd-4088-b516-b21d0acbcfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(tensor):\n",
    "    \"\"\"\n",
    "    Displays images from a 4D Tensor (batch_size, channels, x, y).\n",
    "    \n",
    "    Args:\n",
    "    tensor (torch.Tensor): A tensor containing image data.\n",
    "    \"\"\"\n",
    "    # Check if the tensor requires normalization\n",
    "    if tensor.min() < 0 or tensor.max() > 1:\n",
    "        # Normalize tensor to the range [0, 1]\n",
    "        tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "    \n",
    "    batch_size = tensor.size(0)\n",
    "    fig, axs = plt.subplots(1, batch_size, figsize=(batch_size * 3, 3))\n",
    "    \n",
    "    for i, img in enumerate(tensor):\n",
    "        # Check the number of channels in the image\n",
    "        if img.shape[0] == 3:\n",
    "            # Convert from (channels, x, y) to (x, y, channels) for RGB images\n",
    "            img = img.permute(1, 2, 0)\n",
    "        elif img.shape[0] == 1:\n",
    "            # Squeeze channel dimension for grayscale images\n",
    "            img = img.squeeze(0)\n",
    "        else:\n",
    "            raise ValueError(\"Tensor contains images with unsupported channel size.\")\n",
    "        \n",
    "        # Handle subplots for a batch size of 1\n",
    "        ax = axs[i] if batch_size > 1 else axs\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')  # Hide axes ticks\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16fdda3a-e042-49f2-b965-e52838baec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ade20k'\n",
    "method = 'unimatch_v2'\n",
    "exp = 'dinov2_small'\n",
    "split = '1_2'\n",
    "port = '8080'\n",
    "\n",
    "# Constructing file paths\n",
    "config = f'configs/{dataset}.yaml'\n",
    "labeled_id_path = f'splits/{dataset}/{split}/labeled.txt'\n",
    "unlabeled_id_path = f'splits/{dataset}/{split}/unlabeled.txt'\n",
    "save_path = f'exp/{dataset}/{method}/{exp}/{split}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "766085a7-c0c1-4672-b2b6-7e53ddf0f867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_714057/1426424469.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f'./pretrained/{cfg[\"backbone\"]}.pth')\n",
      "/tmp/ipykernel_714057/1426424469.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  total_state_dict = torch.load('./pretrained/ade20k_unimatch_v2_1_32labels_dinov2_small.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cfg = yaml.load(open(config, \"r\"), Loader=yaml.Loader)\n",
    "\n",
    "# logger = init_log('global', logging.INFO)\n",
    "# logger.propagate = 0\n",
    "\n",
    "# rank, world_size = setup_distributed(port=port)\n",
    "\n",
    "# if rank == 0:\n",
    "#     all_args = {**cfg, **vars(args), 'ngpus': world_size}\n",
    "#     logger.info('{}\\n'.format(pprint.pformat(all_args)))\n",
    "    \n",
    "#     writer = SummaryWriter(args.save_path)\n",
    "    \n",
    "#     os.makedirs(args.save_path, exist_ok=True)\n",
    "\n",
    "cudnn.enabled = True\n",
    "cudnn.benchmark = True\n",
    "\n",
    "model_configs = {\n",
    "    'small': {'encoder_size': 'small', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'base': {'encoder_size': 'base', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'large': {'encoder_size': 'large', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "    'giant': {'encoder_size': 'giant', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
    "}\n",
    "\n",
    "# model = DPT(**{**model_configs[cfg['backbone'].split('_')[-1]], 'nclass': cfg['nclass']}).cuda()\n",
    "model = DPT_with_Feature(**{**model_configs[cfg['backbone'].split('_')[-1]], 'nclass': cfg['nclass']}).cuda()\n",
    "\n",
    "state_dict = torch.load(f'./pretrained/{cfg[\"backbone\"]}.pth')\n",
    "# model.backbone.load_state_dict(state_dict)\n",
    "\n",
    "total_state_dict = torch.load('./pretrained/ade20k_unimatch_v2_1_32labels_dinov2_small.pth')\n",
    "model.load_state_dict(total_state_dict)\n",
    "\n",
    "# if cfg['lock_backbone']:\n",
    "#     model.lock_backbone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58cabb52-7b9b-4be2-9971-1376711a513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ema = deepcopy(model).cuda()\n",
    "model_ema.eval()\n",
    "for param in model_ema.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "if cfg['criterion']['name'] == 'CELoss':\n",
    "    criterion_l = nn.CrossEntropyLoss(**cfg['criterion']['kwargs'])\n",
    "elif cfg['criterion']['name'] == 'OHEM':\n",
    "    criterion_l = ProbOhemCrossEntropy2d(**cfg['criterion']['kwargs'])\n",
    "else:\n",
    "    raise NotImplementedError('%s criterion is not implemented' % cfg['criterion']['name'])\n",
    "\n",
    "class NormalizedCompactnessNormLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NormalizedCompactnessNormLoss, self).__init__()\n",
    "\n",
    "    def forward(self, features_tuple):\n",
    "        # Ensure the input is a tuple with exactly 4 tensors\n",
    "        assert isinstance(features_tuple, tuple) and len(features_tuple) == 4, \"Input must be a tuple of four tensors.\"\n",
    "\n",
    "        # Calculate the normalized norm for each tensor\n",
    "        normalized_norms = [torch.norm(f) / f.numel() for f in features_tuple]\n",
    "\n",
    "        # Compute the mean of these normalized norms\n",
    "        mean_normalized_norm = torch.mean(torch.stack(normalized_norms))\n",
    "\n",
    "        # Return the mean normalized norm as the loss\n",
    "        return mean_normalized_norm\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "class GradientPenaltyLoss(nn.Module):\n",
    "    def __init__(self, entropy_qz=None):\n",
    "        super(GradientPenaltyLoss, self).__init__()\n",
    "        self.entropy_qz = entropy_qz\n",
    "\n",
    "    def forward(self, embeddings, y_pred):\n",
    "        # Initialize total loss to zero\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Iterate over each embedding in the tuple\n",
    "        for embedding in embeddings:\n",
    "            # Ensure that each embedding requires gradient\n",
    "            if not embedding.requires_grad:\n",
    "                raise ValueError(\"Each embedding must require gradients.\")\n",
    "\n",
    "            # Compute squared prediction error\n",
    "            pred_loss = torch.square(y_pred)\n",
    "\n",
    "            # Calculate gradients of pred_loss with respect to this embedding\n",
    "            grad_pred_loss = torch.autograd.grad(outputs=pred_loss, inputs=embedding,\n",
    "                                                 grad_outputs=torch.ones_like(pred_loss),\n",
    "                                                 create_graph=True,allow_unused=True)[0]\n",
    "\n",
    "            # Handle the case where gradients are unused (i.e., None)\n",
    "            if grad_pred_loss is None:\n",
    "                grad_pred_loss = torch.zeros_like(embedding)\n",
    "\n",
    "            # Normalize the gradients\n",
    "            norm = torch.norm(grad_pred_loss, p=2, dim=-1, keepdim=True) + 1e-8\n",
    "            normalized_grad = grad_pred_loss / norm\n",
    "            grad_squared = torch.square(normalized_grad)\n",
    "            \n",
    "            # Apply entropy weighting if provided\n",
    "            if self.entropy_qz is not None:\n",
    "                weighted_grad_squared = self.entropy_qz * grad_squared\n",
    "            else:\n",
    "                weighted_grad_squared = grad_squared\n",
    "            \n",
    "            # Sum the loss over all embeddings\n",
    "            total_loss += torch.mean(weighted_grad_squared)\n",
    "\n",
    "        # Average the loss over the number of embeddings to normalize scale\n",
    "        loss = total_loss / len(embeddings)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "criterion_norm = NormalizedCompactnessNormLoss()\n",
    "criterion_u = nn.CrossEntropyLoss(reduction='none')\n",
    "criterion_gradient = GradientPenaltyLoss()\n",
    "\n",
    "trainset_u = SemiDataset(\n",
    "    cfg['dataset'], cfg['data_root'], 'train_u', cfg['crop_size'], unlabeled_id_path\n",
    ")\n",
    "trainset_l = SemiDataset(\n",
    "    cfg['dataset'], cfg['data_root'], 'train_l', cfg['crop_size'], labeled_id_path, nsample=len(trainset_u.ids)\n",
    ")\n",
    "valset = SemiDataset(\n",
    "    cfg['dataset'], cfg['data_root'], 'val'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dda4ee8-d536-4373-8b92-a1d8573460b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0026, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion_gradient(pred_x_features, pred_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb06de-4a36-4dd5-82e8-4043b207baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_gradient(pred_u_w_features, pred_u_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5edbc53-ca32-47f7-b48f-f702fdd7a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainsampler_l = torch.utils.data.distributed.DistributedSampler(trainset_l)\n",
    "\n",
    "trainloader_l = DataLoader(\n",
    "    trainset_l, batch_size=cfg['batch_size'], pin_memory=True, num_workers=4, drop_last=True, shuffle=False)\n",
    "\n",
    "# trainsampler_u = torch.utils.data.distributed.DistributedSampler(trainset_u)\n",
    "\n",
    "trainloader_u = DataLoader(\n",
    "    trainset_u, batch_size=cfg['batch_size'], pin_memory=True, num_workers=4, drop_last=True, shuffle=False)\n",
    "\n",
    "# valsampler = torch.utils.data.distributed.DistributedSampler(valset)\n",
    "\n",
    "valloader = DataLoader(\n",
    "    valset, batch_size=1, pin_memory=True, num_workers=1, drop_last=False, shuffle=False)\n",
    "\n",
    "total_iters = len(trainloader_u) * cfg['epochs']\n",
    "previous_best, previous_best_ema = 0.0, 0.0\n",
    "best_epoch, best_epoch_ema = 0, 0\n",
    "epoch = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ac4d905-5217-494e-b4b0-7bfb03c85504",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = zip(trainloader_l, trainloader_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8ad6090-0f94-4330-af8b-495e93148083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DPT_with_Feature(\n",
       "  (backbone): DinoVisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x NestedTensorBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MemEffAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (head): DPTHead(\n",
       "    (projects): ModuleList(\n",
       "      (0): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (3): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (resize_layers): ModuleList(\n",
       "      (0): ConvTranspose2d(48, 48, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): ConvTranspose2d(96, 96, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (2): Identity()\n",
       "      (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (scratch): Module(\n",
       "      (layer1_rn): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer2_rn): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer3_rn): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer4_rn): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (refinenet1): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet2): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet3): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet4): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (output_conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 150, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bc21332-d121-4dc1-8ac2-cfecf2f15aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_enumerator = iter(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a00218cb-e0c4-4f44-8dbb-77c0c071aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, ((img_x, mask_x),(img_u_w, img_u_s1, img_u_s2, ignore_mask, cutmix_box1, cutmix_box2)) in enumerate(loader):\n",
    "((img_x, mask_x),(img_u_w, img_u_s1, img_u_s2, ignore_mask, cutmix_box1, cutmix_box2)) = next(loader_enumerator)\n",
    "\n",
    "img_x, mask_x = img_x.cuda(), mask_x.cuda()\n",
    "img_u_w, img_u_s1, img_u_s2 = img_u_w.cuda(), img_u_s1.cuda(), img_u_s2.cuda()\n",
    "ignore_mask, cutmix_box1, cutmix_box2 = ignore_mask.cuda(), cutmix_box1.cuda(), cutmix_box2.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_u_w, pred_u_w_features = model_ema(img_u_w)\n",
    "    pred_u_w = pred_u_w.detach()\n",
    "    conf_u_w = pred_u_w.softmax(dim=1).max(dim=1)[0]\n",
    "    mask_u_w = pred_u_w.argmax(dim=1)\n",
    "\n",
    "img_u_s1[cutmix_box1.unsqueeze(1).expand(img_u_s1.shape) == 1] = img_u_s1.flip(0)[cutmix_box1.unsqueeze(1).expand(img_u_s1.shape) == 1]\n",
    "img_u_s2[cutmix_box2.unsqueeze(1).expand(img_u_s2.shape) == 1] = img_u_s2.flip(0)[cutmix_box2.unsqueeze(1).expand(img_u_s2.shape) == 1]\n",
    "\n",
    "pred_x, pred_x_features = model(img_x)\n",
    "pred_u_s, pred_u_s_features = model(torch.cat((img_u_s1, img_u_s2)), comp_drop=True)\n",
    "\n",
    "pred_u_s1, pred_u_s2 = pred_u_s.chunk(2)\n",
    "# pred_u_s1_features, pred_u_s2_features = pred_u_s_features.chunk(2)\n",
    "\n",
    "mask_u_w_cutmixed1, conf_u_w_cutmixed1, ignore_mask_cutmixed1 = mask_u_w.clone(), conf_u_w.clone(), ignore_mask.clone()\n",
    "mask_u_w_cutmixed2, conf_u_w_cutmixed2, ignore_mask_cutmixed2 = mask_u_w.clone(), conf_u_w.clone(), ignore_mask.clone()\n",
    "\n",
    "mask_u_w_cutmixed1[cutmix_box1 == 1] = mask_u_w.flip(0)[cutmix_box1 == 1]\n",
    "conf_u_w_cutmixed1[cutmix_box1 == 1] = conf_u_w.flip(0)[cutmix_box1 == 1]\n",
    "ignore_mask_cutmixed1[cutmix_box1 == 1] = ignore_mask.flip(0)[cutmix_box1 == 1]\n",
    "\n",
    "mask_u_w_cutmixed2[cutmix_box2 == 1] = mask_u_w.flip(0)[cutmix_box2 == 1]\n",
    "conf_u_w_cutmixed2[cutmix_box2 == 1] = conf_u_w.flip(0)[cutmix_box2 == 1]\n",
    "ignore_mask_cutmixed2[cutmix_box2 == 1] = ignore_mask.flip(0)[cutmix_box2 == 1]\n",
    "\n",
    "loss_x = criterion_l(pred_x, mask_x)\n",
    "loss_x_norm = criterion_norm(pred_x_features)\n",
    "loss_x_gradient = criterion_gradient(pred_x_features, pred_x)\n",
    "\n",
    "loss_u_s1 = criterion_u(pred_u_s1, mask_u_w_cutmixed1)\n",
    "loss_u_s1 = loss_u_s1 * ((conf_u_w_cutmixed1 >= cfg['conf_thresh']) & (ignore_mask_cutmixed1 != 255))\n",
    "loss_u_s1 = loss_u_s1.sum() / (ignore_mask_cutmixed1 != 255).sum().item()\n",
    "\n",
    "loss_u_s2 = criterion_u(pred_u_s2, mask_u_w_cutmixed2)\n",
    "loss_u_s2 = loss_u_s2 * ((conf_u_w_cutmixed2 >= cfg['conf_thresh']) & (ignore_mask_cutmixed2 != 255))\n",
    "loss_u_s2 = loss_u_s2.sum() / (ignore_mask_cutmixed2 != 255).sum().item()\n",
    "\n",
    "loss_u_s = (loss_u_s1 + loss_u_s2) / 2.0\n",
    "\n",
    "loss = (loss_x + loss_x_norm + loss_u_s) / 3.0\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f6855a0-0188-4627-b521-bc4776430b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0026, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_x_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cfe463-fb1c-439f-b76a-3aa945d6270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, features = model(torch.cat((img_u_s1, img_u_s2)), comp_drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f4c51b-1f27-4ae2-83b9-d026f52d9521",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = list(features)\n",
    "feature_tensor = torch.tensor(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e535f2-1ca1-41a3-b263-b4311f4f067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65970215-9163-4cea-a404-d23a4f28545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_norm(pred_x_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a09ce7-cdf9-4253-b967-974bae7b8631",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(img_x.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c371dd6-aa4e-4de1-baa5-5ccb6b5df000",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(mask_x.cpu().unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff223518-4a21-4589-973a-bc3d588da5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(pred_x.argmax(1).detach().cpu().unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae5b8e-1627-4929-b77b-bd4449680bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3495bc87-1329-48d4-9613-365d95e1d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = model.backbone.get_intermediate_layers(\n",
    "            img_x, model.intermediate_layer_idx[model.encoder_size]\n",
    "        )\n",
    "\n",
    "\n",
    "# latent = model.backbone(img_x)\n",
    "noise = np.random.normal(0, 3, latent[0].shape) \n",
    "noisy_latent = []\n",
    "for latent_i in latent:\n",
    "    noisy_latent.append(torch.tensor(latent_i.cpu() + np.float32(noise)).cuda())\n",
    "\n",
    "noisy_latent = tuple(noisy_latent)\n",
    "# noisy_latent = torch.tensor(noisy_latent).cuda()\n",
    "\n",
    "patch_h, patch_w = img_x.shape[-2] // 14, img_x.shape[-1] // 14\n",
    "\n",
    "pred_x_noisy_latent = model.head(noisy_latent, patch_h, patch_w)\n",
    "pred_x_noisy_latent = F.interpolate(pred_x_noisy_latent, (patch_h * 14, patch_w * 14), mode='bilinear', align_corners=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace9e115-7844-46f8-ab9c-c02f3aed4b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743fd4d5-55df-44db-88d7-e23e6d6b298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(noisy_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d960da3b-cbdc-46dc-a3d7-fb739fb4f506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4811494f-5f56-47d6-bd9b-1f6d6c93c259",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccbb9b-6e6d-48bf-a5f6-072e88b20506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each batch\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))  # Prepare a subplot grid\n",
    "batch_titles = ['Batch 1', 'Batch 2']  # Titles for each subplot\n",
    "\n",
    "for i, batch_data in enumerate(noisy_latent):\n",
    "    # Flatten each batch data to [1369, 384]\n",
    "    latent_flat = batch_data.cpu().view(-1, 384)\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    latent_pca = pca.fit_transform(latent_flat.numpy())\n",
    "\n",
    "    # t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "    latent_tsne = tsne.fit_transform(latent_flat.numpy())\n",
    "\n",
    "    # Plot PCA\n",
    "    ax = axes[i, 0]\n",
    "    ax.scatter(latent_pca[:, 0], latent_pca[:, 1], alpha=0.5)\n",
    "    ax.set_title(f'PCA of {batch_titles[i]}')\n",
    "\n",
    "    # Plot t-SNE\n",
    "    ax = axes[i, 1]\n",
    "    ax.scatter(latent_tsne[:, 0], latent_tsne[:, 1], alpha=0.5)\n",
    "    ax.set_title(f't-SNE of {batch_titles[i]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78834c9d-9c18-4f3a-b7cd-24a90192be15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
